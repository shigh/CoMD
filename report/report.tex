\documentclass[12pt]{article}
\usepackage[margin=1.0in]{geometry}

%\usepackage[utf8]{inputenc}
%\usepackage[T1]{fontenc}
%\usepackage{fixltx2e}
%\usepackage{longtable}
%\usepackage{float}
%\usepackage{wrapfig}
%\usepackage{rotating}
%\usepackage[normalem]{ulem}
%\usepackage{textcomp}
%\usepackage{marvosym}
%\usepackage{wasysym}
%\tolerance=1000

\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsfonts}
\usepackage{bm}
\usepackage{algpseudocode}
\usepackage{hyperref}
\usepackage{booktabs}

\usepackage{listings}

\lstset{language=C}
\newcommand{\pd}[2]{\frac{\partial #1}{ \partial #2}}
\renewcommand{\v}[1]{\bold{#1}}

\title{CS 598 Final Project}
\author{Scott High and Erin Molloy}
\date{\today}


\begin{document}

\maketitle

\section{Introduction}
Molecular dynamics simulations are important for researching physical phenomena 
from the electronic structure of metal to the folding trajectories of proteins. 
In our final project, we create a performance expectation for a computationally
intensive compute function in a molecular dynamics simulation mini-app, 
called \href{https://github.com/exmatex/CoMD}{\texttt{CoMD}}, 
which is designed to expose core features of molecular dynamics simulations 
in a code base that is simple to understand and modify \cite{CoMD}. Our contributions include
\begin{itemize}
    \item [(1)] A {\bf performance expectation} for the Lennart-Jones force computation function.
    \item [(2)] Simple code {\bf modifications} which improve performance by xx-xx\%.
    \item [(3)] Suggestions for more involved modifications to greater improve performance.
    \item [(4)] An illustration that {\bf compiler choice} impacts performance of the compute function.
\end{itemize}

\section{Background}
Molecular dynamics simulations model the movement of individual particles over time. 
Particle motion is determined by Newton's well-known equation of motion, $F = ma$. 
In an $N$-particle simulation, the force acting on particle $i$ at position 
$\bm{r}_i = (x_i, y_i, z_i)$ is given by
\begin{align*}
    \bm{f}_i = m_i \ddot{\bm{r}}_i = -\frac{\partial}{\partial \bm{r}_i} U(\bm{r}_1, \dots, \bm{r}_N)
\end{align*}
where $U$ is the potential energy from particle-particle interactions \cite{Intro}.
The resulting system of first order ODEs is integrated to find the particle positions
at each time step. The \texttt{CoMD} mini-app computes force using the truncated 
Lennart-Jones, where the the potential between a pair of particles, call $i$ and $j$,
is given by
\begin{align*}
U_{LJ}(r_{ij}) := 
\left\{ \begin{array}{l c} 
    4 \epsilon \left[ \left(\frac{\sigma}{r_{ij}}\right)^{12} - 
                     \left(\frac{\sigma}{r_{ij}}\right)^6 \right] & r_{ij} \le r_c \\
     0 & r_{ij} > r_c
    \end{array} \right.
\end{align*}
where
\begin{align*}
    r_{ij} = \sqrt{(x_i - x_j)^2 + (y_i - y_j)^2 + (z_i - z_j)^2}, 
\end{align*}
is the distance between particles and $r_c = 2.5 \sigma$ is the cut-off radius for 
particle-particle interactions. Thus, the cut-off radius defines a sphere with volume 
$(4/3) \pi r_c^3$ within which pair potentials must be computed. Material parameters 
$\epsilon$ and $\sigma$ are the potential well depth and distance at which the 
pair potential becomes zero, respectively \cite{Wiki}. 

\section{Performance baseline}
\texttt{CoMD} includes a default simulation of Copper atoms in a face-centered 
cubic (FCC) lattice with a spacing of $3.615 \text{ \AA}$ and material parameters of 
$\epsilon = 0.167 \text{ eV}$ and $\sigma = 2.315 \text{ \AA}$ \cite{CoMD}. 
Thus, there exist
\begin{align*}
     \bigg( \frac{4}{3} \pi (2.5 \cdot 2.315)^3 \text{ \AA}^3 \bigg) \times
     \bigg( \frac{4 \text{ particles}}{3.615 \text{ \AA} \times 3.615 \text{ \AA} \times 3.615 \text{ \AA}} \bigg)
     = 69 \text{ particles}
\end{align*}
within the cut-off radius of each particle at lattice initialization. 

In our scaling study, we use these default parameters with a simulation
temperatures of 0, 600, and 3000 K. Strong scaling studies with16,384,000 atoms
are performed on 64, 512, and 4096 processors. Weak scaling studies are
performed with 32000 atoms per processor for 1, 8, 64, 512, and 4096 processors.
As the force computation function dominates the total runtime, we target the force computation 
function to improve the overall performance of \texttt{CoMD} (Table 1).

A performance baseline is obtained by running \texttt{CoMD} on a single processor
and increasing the size of the lattice by a factor of 10 in each dimension.
Default parameters are used with a temperature of 0 K, which results in particles 
remaining in their initial positions throughout the simulation. This enables us to 
model the number of particle-particle interactions as the constant value.

\begin{table}[h!]
\centering
\begin{tabular}{c c | c c | c c}
\toprule
& \multicolumn{1}{c}{} & \multicolumn{2}{c}{Strong scaling} & \multicolumn{2}{c}{Weak scaling} \\
\cmidrule(r){3-4} \cmidrule(r){5-6}
\multicolumn{1}{c}{} & \multicolumn{1}{c}{}  & \multicolumn{1}{c}{force} & \multicolumn{1}{c}{halo} & \multicolumn{1}{c}{force} & \multicolumn{1}{c}{halo} \\
\multicolumn{1}{c}{temperature} & \multicolumn{1}{c}{\# ranks} & \multicolumn{1}{c}{computation} & \multicolumn{1}{c}{exchange} & \multicolumn{1}{c}{computation} & \multicolumn{1}{c}{exchange} \\
\midrule
 & 1 & - & - & 5.1e-1/97.4\% & - \\ 
& 8 & - & - & 6.2e-1/96.5\% & 1.2e-3/0.2\% \\ 
$T = 0 K$ & 64 & 4.8e-0/96.8\% & 2.2e-2/0.4\% & 6.2e-1/96.3\% & 3.9e-3/0.6\% \\ 
& 512 & 6.2e-1/85.9\% & 8.2e-2/11.4\% & 6.2e-1/91.4\% & 5.9e-3/0.9\% \\ 
& 4096 & 8.3e-2/72.3\% & 1.8e-2/15.7\% & 6.2e-1/91.9\% & 9.0e-3/1.3\% \\ 
\midrule
 & 1 & - & - & 5.0e-1/97.4\% & - \\ 
& 8 & - & - & 6.1e-1/96.4\% & 1.4e-3/0.2\% \\ 
$T = 600 K$ & 64 & 4.8e-0/94.9\% & 9.2e-2/1.8\% & 6.1e-1/96.2\% & 3.6e-3/0.6\% \\ 
& 512 & 6.1e-1/95.9\% & 5.7e-3/0.9\% & 6.1e-1/95.9\% & 5.9e-3/0.9\% \\ 
& 4096 & 8.1e-2/67.9\% & 3.6e-3/3.0\% & 6.1e-1/92.0\% & 9.1e-3/1.4\% \\ 
\midrule
 & 1 & - & - & 5.0e-1/97.3\% & - \\ 
& 8 & - & - & 6.3e-1/96.5\% & 1.0e-3/0.2\% \\ 
$T = 3000 K$ & 64 & 4.7e-0/96.7\% & 2.3e-2/0.5\% & 6.0e-1/96.2\% & 4.2e-3/0.7\% \\ 
& 512 & 6.0e-1/95.9\% & 6.1e-3/1.0\% & 6.0e-1/95.9\% & 6.3e-3/1.0\% \\ 
& 4096 & 8.1e-2/66.3\% & 3.0e-3/2.4\% & 6.1e-1/96.4\% & 9.6e-3/1.5\% \\ 
\bottomrule
\end{tabular}
\caption{Scaling Studies (seconds spent in function/percentage of total runtime)}
\end{table}

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.45\textwidth]{../figs/baseline_forceLJ.pdf}
    \caption{Time spent in force computation function on a single processor}
\end{figure}

\newpage

\section{Performance model}
All analysis is done assuming a one-level cache model and
approximating read and write times as equal.  The constants used in
the performance expectations are determined by the clock rate and
\texttt{STREAM} benchmark results as measured on Blue Waters.

The equations we are calculating for each particle in computer variables are
\begin{align}
  E_{tot} &= \sum_{ij} U_{LJ}(r_{ij}) \\
  U_{LJ}(r_{ij}) &= 
  A\left(\frac{1}{r_{ij}}\right)^{6}\left\{ \left(\frac{1}{r_{ij}}\right)^{6} - 1 \right\} \\
  \notag\\
  \textbf{F}(r_{ij}) &= - U'_{LJ}(r_{ij})\hat{r}_{ij} \notag\\
      &=  24 \frac{\epsilon}{r_{ij}} \left\{ 2 \left(\frac{\sigma}{r_{ij}}\right)^{12}
              - \left(\frac{\sigma}{r_{ij}}\right)^6 \right\} \hat{r}_{ij} \notag\\
              &=  A \frac{1}{r_{ij}^2} \left(\frac{1}{r_{ij}}\right)^{6} \left\{ 2 \left(\frac{1}{r_{ij}}\right)^{6}
              - 1 \right\} \textbf{r}_{ij}
\end{align}
in a fully periodic domain. The total number of simulation particles
is $N$, and each particle has $n$ neighbors within
$r_c=r_{\text{cutoff}}$.

We develope our performance model by calculating the cost of updating
particle $i$ and its interaction with each of its $n$ neighbors
$j$. The cost is:
\begin{itemize}
\item[A)] 3 loads for particle $i$'s position
\item[B)] For each particle $j$ where $r_{ij}<r_c$
  \begin{enumerate}
    \item $3$ loads for particle $j$'s position
    \item 3 subtractions, 3 multiplications, 2 additions and 1 division to calculate $\frac{1}{r_{ij}^2}$ 
    \item 3 multiplications to calculate $\left(\frac{1}{r_{ij}^2}\right)^3=\left(\frac{1}{r_{ij}}\right)^6$
    \item 2 subtractions and 2 multiplications to calculate $U_{LJ}$ (1 subtraction is for cutoff potential)
    \item 3 multiplications and 1 subtraction to calclulate $F(r_{ij})$
    \item 3 multiplications and 3 additions to calculate and update $F_{x,y,z}$
  \end{enumerate}
\item[C)] 1 write to save $U_{LJ}$. (Assume we can save the $U_{LJ}(r_{ji})$ term for free)
\item[D)] 3 writes to save $F_{x,y,z}$
\item[E)] 1 addition to update $E$
\end{itemize}
The total cost is $N(A+nB/2)+C+D+E$, where the $1/2$ is to account for
double counting. Using $c$ for the cost of a floating point operation,
$w$ for the cost of a write and $r$ for the cost of a read the
estimated cost is
\begin{equation}
N \left(\frac{n}{2} \left(26 c + 3 w\right) + 3 w\right) + c + 4 r
\end{equation}
Approximating reads and writes as equal reduces this to
\begin{equation}
  N \left(\frac{n}{2} \left(26 c + 3 r\right) + 3 r\right) + c + 4 r
  \label{eqn:perf-model}
\end{equation}
We approximate $n$ by noting that the density of atoms in an FCC
lattice is 4 atoms/cell, so the average number of particles within in
a sphere of radius $r_c$ is $n \approx 4\frac{4\pi}{3}r_c^3\approx69$.
We approximate $c=4.35\times10^{-10}\;\text{flops}/s$ using the clock
rate and $r=1.43\times10^{-9}\;\text{B}/s$ using the STREAM benchmark
on Bluewaters. The results of this performance model are shown as
model 1 in figure \ref{fig:perf-models}.

The data structure used for storing the computational particles is a
struct of linear C arrays. Each linear array is divided into equal
sized sections which store information for particles in a region of
the simulation domain. The size of the sections is the maximum number
of particles per region of space, and does not change throughout the
simulation. This ordering of particles is used to efficently determine
which particle pairs are within the cutoff radius.

This performance model represents a best case scenario for the number
of operations required to compute the inter-particle forces. To better
test the implementation we also developed performance models that
match the algorithm used. This requires adding a term to take into
account the work required to check if a particle is within
$r_c$. Denote the number of particles checked that are not within
$r_c$ as $k$. Each check requires 3 reads, 3 subtractions, 3
multiplications and 2 additions to calculation $\| x_i-x_j\|_2^2$,
giving us the performanced model
\begin{equation}
  N \left(\frac{n}{2} \left(26 c + 3 r\right)+\frac{k}{2} \left(8 c + 3 r\right) + 3 r\right) + c + 4 r
  \label{eqn:perf-model-nk}
\end{equation}
The actual implementation does not compute interactions and distance
checks only once, but rather does so for each particle. This doubles
the number of interactions and distance checks over the performance
model \ref{eqn:perf-model-nk} to give
\begin{equation}
  N \left(n \left(26 c + 3 r\right)+k \left(8 c + 3 r\right) + 3 r\right) + c + 4 r
  \label{eqn:perf-model-2n2k}
\end{equation}

% include picture of particle lattice...

% Graphs -- Erin

\begin{figure}[h!]
  \centering
  \includegraphics[width=0.5\textwidth]{../figs/perfmodel_forceLJ}
  \caption{Performance models and baseline results. Model 1 is
    equation \ref{eqn:perf-model} with $n=69$ and model 2 is
    \ref{eqn:perf-model} with $n=2\times69$.}
  \label{fig:perf-models}
\end{figure}

\section{Performance Baseline}
% Erin Molloy

A baseline for the force routine was established by running the code
on a single processor with *settings* using the default Cray
compiler. This was then compared with the performance models
\ref{eqn:perf-model}-\ref{eqn:perf-model-2n2k} developed above. The
results are shown in figure \ref{fig:perf-models}. Clearly, the
baseline using the Cray compiler does not agree well with the results
of our performance models. We were unable to determine why the Cray
compiler has such low performance, and ran additional baselines using
code compiled with gcc 4.7.1. The baseline run with gcc gave excellent
agreement with our performance model \ref{eqn:perf-model-2n2k} which
was developed to describe the algorithim actually used.

\subsection{Strong scaling}

\subsection{Weak scaling}

\section{Modifications}
\label{sec:mods}

\begin{figure}[h!]
  \centering
  \includegraphics[width=\textwidth]{../figs/modified_forceLJ}
  \caption{Runtime and speedups from the modifications described in
    section \ref{sec:mods}}
  \label{fig:mod-force}
\end{figure}

The performance baseline revelead a large discrepancy between our
performance model and experimental results for the Cray compiler. We
have focused our modifications on attempting to improve the results
using the Cray compiler. The results using the gcc compiler agree well
with our performance model, and further improvements would require
changes to the underlying data structures or a complete rewrite of the
force calculation routines, both of which are beyond the scope of this
project.

The first thing we noticed while analyzing the source code is a large
number of pointer dereferences to access the arrays storing particle
information. From the code report generated by the Cray compiler it
was clear that these dereferences were being done everytime the
pointers were accessed in the innermost loop of the force
calcuation. A simple optimzation was then to derefernce these pointers
before the computation. The small loops over real3 variables are also
not unrolled by compiler, so we did this by hand. The inner loop of
the force calculation routine is shown before and after optimization
in appendix \ref{apd:code-changes}.

The Cray compiler reports tell us that no vectorization is occuring in
the force calculation. After investgating we determined that this is
in fact the correct behavior, as the data structure used does not
guarantee that the arrays used are not aliased. All particle
information is stored in contigous arrays and aliasing occurs when
calculating the interactions between particles in the same cell. We
considered modifiying the data structure to remove aliasing, but
determined that the modifications required are beyond the scope of
this project.

Another potential improvement considered was changing the order of
particles within a cell to improve memory access patterns. The code
currently sorts the particles within a cell regularly. Because of this
it is unlikely that substational improvements could be made.

After the above modification were made the runtimes using the Cray
compiler were still several orders of magnitude higher than our
performance expectations. With no clear reason for this discrepency we
ran our single core bencharks using code compiled with gcc and found
that the runtime decreased dramatically. From the gcc compiler report
it was clear that no additional vectorization was occuring. We have
been unable to determine the reason for the large difference between
compilers.

\section{Conclusion}
% REFERENCES???



\appendix
\section{Code changes}
\label{apd:code-changes}
The changes made to the inner loop in the force cacluation are shown
here.
\subsection{Initial Inner Force Loop}
\begin{lstlisting}
  real_t dr[3];
  int jId = s->atoms->gid[jOff];  
  if (jBox < s->boxes->nLocalBoxes && jId <= iId )
  continue; // don't double count local-local pairs.
  real_t r2 = 0.0;
  for (int m=0; m<3; m++)
  {
    dr[m] = s->atoms->r[iOff][m]-s->atoms->r[jOff][m];
    r2+=dr[m]*dr[m];
  }

  if ( r2 > rCut2) continue;

  r2 = 1.0/r2;
  real_t r6 = s6 * (r2*r2*r2);
  real_t eLocal = r6 * (r6 - 1.0) - eShift;
  s->atoms->U[iOff] += 0.5*eLocal;
  s->atoms->U[jOff] += 0.5*eLocal;

  if (jBox < s->boxes->nLocalBoxes)
  ePot += eLocal;
  else
  ePot += 0.5 * eLocal;

  real_t fr = - 4.0*epsilon*r6*r2*(12.0*r6 - 6.0);
  for (int m=0; m<3; m++)
  {
    s->atoms->f[iOff][m] -= dr[m]*fr;
    s->atoms->f[jOff][m] += dr[m]*fr;
  }
\end{lstlisting}

\subsection{Optimized Inner Force Loop}
\begin{lstlisting}
  real_t dr[3];
  int jId = gid[jOff];  
  if (jBox < nLocalBoxes && jId <= iId )
  continue; // don't double count local-local pairs.

  real_t r2 = 0.0;
  dr[0] = r_iOff[0]-r[jOff][0];
  r2+=dr[0]*dr[0];
  dr[1] = r_iOff[1]-r[jOff][1];
  r2+=dr[1]*dr[1];
  dr[2] = r_iOff[2]-r[jOff][2];
  r2+=dr[2]*dr[2];

  if ( r2 > rCut2) continue;

  r2 = 1.0/r2;
  real_t r6 = s6 * (r2*r2*r2);
  real_t eLocal = r6 * (r6 - 1.0) - eShift;
  U[iOff] += 0.5*eLocal;
  U[jOff] += 0.5*eLocal;

  if (jBox < nLocalBoxes)
  ePot += eLocal;
  else
  ePot += 0.5 * eLocal;

  real_t fr = - 4.0*epsilon*r6*r2*(12.0*r6 - 6.0);
  f_iOff[0] -= dr[0]*fr;
  f[jOff][0] += dr[0]*fr;
  f_iOff[1] -= dr[1]*fr;
  f[jOff][1] += dr[1]*fr;
  f_iOff[2] -= dr[2]*fr;
  f[jOff][2] += dr[2]*fr;
\end{lstlisting}

\end{document}
